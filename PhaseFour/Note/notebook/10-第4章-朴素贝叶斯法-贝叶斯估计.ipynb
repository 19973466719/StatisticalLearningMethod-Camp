{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 第4章-朴素贝叶斯法-贝叶斯估计\n",
    "\n",
    "&emsp;&emsp;本章中对$Y$的分布的估计，以及$Y$的条件下$X$分布的估计，用到了两种估计方法：极大似然估计和贝叶斯估计。本节通过对$Y$分布的估计来介绍一下极大似然估计和贝叶斯估计得到的结果。  \n",
    "\n",
    "### 问题描述\n",
    "&emsp;&emsp;随机变量$Y$的分布是离散的，可能的取值为$c_1,c_2,\\dots,c_K$，$Y$属于多项分布（对于每一个类别，$Y$的取值都有一个对应的概率$\\theta_i$），可知$\\sum \\theta_i=1$。  \n",
    "&emsp;&emsp;下面用掷硬币的例子，$Y$出现的结果是正面或反面，正面出现的概率记为$\\theta_1$，反面出现的概率记为$\\theta_2$，$\\theta_1+\\theta_2=1$，如果只有两个结果，$Y$为二项分布，如果有$K$个结果，叫做多项分布。将K个$\\theta$写在一起，记为向量$\\hat{\\theta}$。  \n",
    "$\\therefore Y$的分布为$P(Y=y|\\theta)=\\theta_1^{I\\{y=c_1\\}} \\cdot \\theta_2^{I\\{y=c_2\\}} \\dots \\theta_K^{I\\{y=c_K\\}}$  \n",
    "\n",
    "### 极大似然估计\n",
    "&emsp;&emsp;在极大似然估计中，得到了$y_1,y_2,\\dots,y_N$样本，可得到联合概率分布：其中$m_i$表示出现结果是$c_i$的次数\n",
    "$$\\begin{aligned} \n",
    "P(y_1,y_2,\\dots,y_N | \\theta) \n",
    "&= P(y_1|\\theta)P(y_2|\\theta) \\dots P(y_N|\\theta) \\\\\n",
    "&= \\theta_1^{m_1}\\theta_2^{m_2} \\dots \\theta_N^{m_N} \\\\\n",
    "\\end{aligned}$$\n",
    "极大似然估计的思想是通过最大化这个联合概率分布，寻找$\\theta$的估计值：\n",
    "$$\\max P(y_1,y_2,\\dots,y_N | \\theta) \\Rightarrow \\max lnP(\\cdot) = m_1 \\ln \\theta_1 + m_2 \\ln \\theta_2 + \\dots + m_K \\ln \\theta_K$$\n",
    "$\\because \\theta_1+\\theta_2+\\dots+\\theta_K=1$  \n",
    "为求上式，需要引入拉格朗日乘子：$\\displaystyle \\max_{\\theta_1,\\dots,\\theta_K} m_1 \\ln \\theta_1 + m_2 \\ln \\theta_2 + \\dots + m_K \\ln \\theta_K + \\lambda(\\theta_1+\\dots+\\theta_K-1)$  \n",
    "对上式求偏导可得：  \n",
    "$\\begin{array}{c} \n",
    "\\frac{m_1}{\\theta_1}+\\lambda = 0 \\Rightarrow \\theta_1 = -\\frac{m_1}{\\lambda} \\\\\n",
    "\\frac{m_2}{\\theta_2}+\\lambda = 0 \\Rightarrow \\theta_2 = -\\frac{m_2}{\\lambda}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{m_K}{\\theta_K}+\\lambda = 0 \\Rightarrow \\theta_K = -\\frac{m_K}{\\lambda}\n",
    "\\end{array}$\n",
    "$\\because \\theta_1+\\theta_2+\\dots+\\theta_K=1$  \n",
    "$\\therefore -\\frac{m_1+m_2+\\dots+m_K}{\\lambda}=1 \\Rightarrow \\lambda = -(m_1+m_2+\\dots+m_K)$，将该结果带入前面公式  \n",
    "$\\therefore \\begin{array}{c} \n",
    "\\theta_1 = -\\frac{m_1}{\\lambda} = \\frac{m_1}{N} \\\\\n",
    "\\theta_2 = -\\frac{m_2}{\\lambda} = \\frac{m_2}{N} \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_K = -\\frac{m_K}{\\lambda} = \\frac{m_K}{N}\n",
    "\\end{array}$  \n",
    "\n",
    "### 贝叶斯估计  \n",
    "&emsp;&emsp;又回顾掷硬币的例子，结果为$y \\in \\{正,反\\}$，$\\theta$的先验概率分布是B（贝塔）分布，正面出现的概率为$\\theta_1$，反面出现的概率为$\\theta_2$，$p(\\theta)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta_1^{\\alpha-1}\\theta_2^{\\beta-1}$  \n",
    "&emsp;&emsp;如果$y \\in \\{c_1,\\dots,c_K\\}$，对应的概率为$\\theta_1,\\dots,\\theta_K$，$\\theta$的先验概率分布是Dirichlet分布，$p(\\theta)=\\frac{\\Gamma(\\alpha_1+\\dots+\\alpha_K)}{\\Gamma(\\alpha_1)\\dots\\Gamma(\\alpha_K)}\\theta_1^{\\alpha-1}\\theta_2^{\\alpha-2}\\dots\\theta_K^{\\alpha-K}$  \n",
    "&emsp;&emsp;在Beta分布中，当$\\alpha=\\beta>1$时，$\\theta_1=\\frac{1}{2}$的概率最大，在先验信息中，认为正面和反面出现的概率，很大的可能是相等的。同样在Dirichlet分布中，$\\alpha_1=\\alpha_2=\\dots=\\alpha_K=\\alpha$，也满足如上假设。  \n",
    "&emsp;&emsp;获得样本之后，求取$\\theta$的后验概率分布  \n",
    "$$\\begin{aligned} p(\\theta|y_1,\\dots,y_N)\n",
    "&= \\frac{p(\\theta,y_1,\\dots,y_N)}{p(y_1,\\dots,y_N)} \\\\\n",
    "&\\propto p(\\theta)p(y_1,\\dots,y_N | \\theta) \\\\\n",
    "&\\propto \\theta_1^{\\alpha-1}\\theta_2^{\\alpha-1}\\dots\\theta_K^{\\alpha-1}\\cdot\\theta_1^{m_1}\\theta_2^{m_2}\\dots\\theta_K^{m_K} \\\\\n",
    "&\\propto \\theta_1^{m_1+\\alpha-1}\\theta_2^{m_2+\\alpha-1}\\dots\\theta_K^{m_K+\\alpha-1}\n",
    "\\end{aligned}$$\n",
    "&emsp;&emsp;上述是一个Dirichlet分布，为什么会是这个分布呢？正如看到$e^{-\\theta^2+a\\theta+b}$这种形式，是正态分布的感觉是一样的。  \n",
    "根据上面的公式，可以写出系数：\n",
    "$\\frac{\\Gamma(m_1+m_2+\\dots+m_K+K\\alpha)}{\\Gamma(m_1+\\alpha)\\Gamma(m_2+\\alpha)\\dots\\Gamma(m_K+\\alpha)}$  \n",
    "$\\therefore p(\\theta|y_1,\\dots,y_N)=\\frac{\\Gamma(m_1+m_2+\\dots+m_K+K\\alpha)}{\\Gamma(m_1+\\alpha)\\Gamma(m_2+\\alpha)\\dots\\Gamma(m_K+\\alpha)}\\theta_1^{m_1+\\alpha-1}\\theta_2^{m_2+\\alpha-1}\\dots\\theta_K^{m_K+\\alpha-1}$  \n",
    "&emsp;&emsp;由于计算贝叶斯估计是计算后验概率最大值，不需要如此精确的值，所以计算的时候去掉前面的系数，即$\\max \\theta_1^{m_1+\\alpha-1}\\theta_2^{m_2+\\alpha-1}\\dots\\theta_K^{m_K+\\alpha-1}$  \n",
    "最大后验结果为：  \n",
    "$$\\begin{aligned}\\theta_1 \n",
    "&= \\frac{m_1+\\alpha-1}{m_1+m_2+\\dots+m_K+K(\\alpha-1)} \\\\\n",
    "&= \\frac{m_1+\\alpha-1}{N+K\\alpha-K}\n",
    "\\end{aligned}$$\n",
    "易得：$\\theta_2=\\frac{m_2+\\alpha-1}{N+K\\alpha-K}$，后面省略。  \n",
    "所以对应书上第51页中：$\\lambda=\\alpha-1$  \n",
    "\n",
    "### 总结\n",
    "&emsp;&emsp;这一部分所讲的内容，属于书上的扩展内容，比较超纲，如果没有看懂，也没有关系，可以尝试地推导一下，如果学有余力，可以学习扩展内容：LDA(Latent Dirichlet Alocation)模型，这个模型的基础就是Dirichlet分布和贝叶斯框架，感兴趣的同学可以查一下相关资料。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
